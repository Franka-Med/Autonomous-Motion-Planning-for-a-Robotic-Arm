{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defination of basic environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mujoco\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "\n",
    "from gymnasium.core import ObsType\n",
    "from gymnasium_robotics.envs.robot_env import MujocoRobotEnv\n",
    "from gymnasium_robotics.utils import rotations\n",
    "from typing import Optional, Any, SupportsFloat\n",
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3 import SAC,DDPG,HerReplayBuffer\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from sb3_contrib.tqc import TQC\n",
    "from torchviz import make_dot\n",
    "from IPython.display import display\n",
    "\n",
    "# camera configuration parameters\n",
    "DEFAULT_CAMERA_CONFIG = {\n",
    "    \"distance\": 2.5,\n",
    "    \"azimuth\": 135.0,\n",
    "    \"elevation\": -20.0,\n",
    "    \"lookat\": np.array([0.0, 0.5, 0.0]),\n",
    "}\n",
    "\n",
    "class FrankaEnv(MujocoRobotEnv):\n",
    "#########################  Initialization  ################################\n",
    "    #rendering the environment\n",
    "    metadata = {\n",
    "        \"render_modes\": [\n",
    "            \"human\",\n",
    "            \"rgb_array\",\n",
    "        ],# for visualizarion and code analysis\n",
    "        \"render_fps\": 20,\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str = None,\n",
    "        n_substeps: int = 50,\n",
    "\n",
    "        # reward and robot sets\n",
    "        reward_type: str = \"sparse\",\n",
    "        block_gripper: bool = False,\n",
    "        distance_threshold: float = 0.05,\n",
    "        \n",
    "        # position range of the goal and object\n",
    "        goal_xy_range: float = 0.3,\n",
    "        obj_xy_range: float = 0.3,\n",
    "        goal_x_offset: float = 0.4,\n",
    "        goal_z_range: float = 0.2,\n",
    "                \n",
    "        **kwargs,\n",
    "    ):\n",
    "        # gripper management and model save\n",
    "        self.block_gripper = block_gripper\n",
    "        self.model_path = model_path\n",
    "        \n",
    "        # space of robot action: x y z -> 3 & unlocking gripper for 4\n",
    "        action_size = 3\n",
    "        action_size += 0 if self.block_gripper else 1 \n",
    "        \n",
    "        # definition of robot joint\n",
    "        self.reward_type = reward_type\n",
    "        self.neutral_joint_values = np.array([0.00, 0.41, 0.00, -1.85, 0.00, 2.26, 0.79, 0.00, 0.00])\n",
    "        \n",
    "        # specific parameter\n",
    "        super().__init__(\n",
    "            n_actions=action_size,\n",
    "            n_substeps=n_substeps,\n",
    "            model_path=self.model_path,\n",
    "            initial_qpos=self.neutral_joint_values,\n",
    "            default_camera_config=DEFAULT_CAMERA_CONFIG,\n",
    "            **kwargs,\n",
    "        )\n",
    "        \n",
    "        self.distance_threshold = distance_threshold\n",
    "        \n",
    "        # settings of sample areas for the object and goal\n",
    "        self.obj_xy_range = obj_xy_range\n",
    "        self.goal_xy_range = goal_xy_range\n",
    "        self.goal_x_offset = goal_x_offset\n",
    "        self.goal_z_range = goal_z_range\n",
    "\n",
    "        # range\n",
    "        self.goal_range_low = np.array([-self.goal_xy_range / 2 + goal_x_offset, -self.goal_xy_range / 2, 0])\n",
    "        self.goal_range_high = np.array([self.goal_xy_range / 2 + goal_x_offset, self.goal_xy_range / 2, self.goal_z_range])\n",
    "        self.obj_range_low = np.array([-self.obj_xy_range / 2, -self.obj_xy_range / 2, 0])\n",
    "        self.obj_range_high = np.array([self.obj_xy_range / 2, self.obj_xy_range / 2, 0])\n",
    "         \n",
    "        self.goal_range_low[0] += 0.6\n",
    "        self.goal_range_high[0] += 0.6\n",
    "        self.obj_range_low[0] += 0.6\n",
    "        self.obj_range_high[0] += 0.6\n",
    "\n",
    "        # properities of model\n",
    "        self.nu = self.model.nu\n",
    "        self.nq = self.model.nq\n",
    "        self.nv = self.model.nv\n",
    "        self.ctrl_range = self.model.actuator_ctrlrange\n",
    "        \n",
    "    def _initialize_simulation(self) -> None:\n",
    "        # loading the model and data\n",
    "        self.model = self._mujoco.MjModel.from_xml_path(self.fullpath)\n",
    "        self.data = self._mujoco.MjData(self.model)\n",
    "\n",
    "        # obtain model & size\n",
    "        self._model_names = self._utils.MujocoModelNames(self.model)\n",
    "        self.model.vis.global_.offwidth = self.width\n",
    "        self.model.vis.global_.offheight = self.height\n",
    "        \n",
    "        # obtain joint and gripper\n",
    "        free_joint_index = self._model_names.joint_names.index(\"obj_joint\")\n",
    "        self.arm_joint_names = self._model_names.joint_names[:free_joint_index][0:7]\n",
    "        self.gripper_joint_names = self._model_names.joint_names[:free_joint_index][7:9]\n",
    "        \n",
    "        # setting the simulation environment & time & velocity\n",
    "        self._env_setup(self.neutral_joint_values)\n",
    "        self.initial_time = self.data.time\n",
    "        self.initial_qvel = np.copy(self.data.qvel)\n",
    "\n",
    "    def _env_setup(self, neutral_joint_values) -> None:\n",
    "        self.set_joint_neutral()\n",
    "        self.data.ctrl[0:7] = neutral_joint_values[0:7]\n",
    "        self.reset_mocap_welds(self.model, self.data)\n",
    "        self._mujoco.mj_forward(self.model, self.data)\n",
    "        self.initial_mocap_position = self._utils.get_site_xpos(self.model, self.data, \"ee_center_site\").copy()\n",
    "        self.grasp_site_pose = self.get_ee_orientation().copy()\n",
    "        self.set_mocap_pose(self.initial_mocap_position, self.grasp_site_pose)\n",
    "        self._mujoco_step()\n",
    "        self.initial_object_height = self._utils.get_joint_qpos(self.model, self.data, \"obj_joint\")[2].copy()\n",
    "\n",
    "##############################  Info  #####################################\n",
    "    \n",
    "    def goal_distance(self, goal_a, goal_b) -> SupportsFloat:\n",
    "        assert goal_a.shape == goal_b.shape\n",
    "        return np.linalg.norm(goal_a - goal_b, axis=-1)\n",
    "    \n",
    "    def get_ee_orientation(self) -> np.ndarray:\n",
    "        site_mat = self._utils.get_site_xmat(self.model, self.data, \"ee_center_site\").reshape(9, 1)\n",
    "        current_quat = np.empty(4)\n",
    "        self._mujoco.mju_mat2Quat(current_quat, site_mat)\n",
    "        return current_quat\n",
    "\n",
    "    def get_ee_position(self) -> np.ndarray:\n",
    "        return self._utils.get_site_xpos(self.model, self.data, \"ee_center_site\")\n",
    "\n",
    "    def get_body_state(self, name) -> np.ndarray:\n",
    "        body_id = self._model_names.body_name2id[name]\n",
    "        body_xpos = self.data.xpos[body_id]\n",
    "        body_xquat = self.data.xquat[body_id]\n",
    "        body_state = np.concatenate([body_xpos, body_xquat])\n",
    "        return body_state\n",
    "\n",
    "    def get_fingers_width(self) -> np.ndarray:\n",
    "        finger1 = self._utils.get_joint_qpos(self.model, self.data, \"finger_joint1\")\n",
    "        finger2 = self._utils.get_joint_qpos(self.model, self.data, \"finger_joint2\")\n",
    "        return finger1 + finger2\n",
    "    \n",
    "##############################  Sample  ###################################\n",
    "    def _sample_goal(self) -> np.ndarray:\n",
    "        goal = np.array([0.0, 0.0, self.initial_object_height])\n",
    "        noise = self.np_random.uniform(self.goal_range_low, self.goal_range_high)\n",
    "        if not self.block_gripper and self.goal_z_range > 0.0:\n",
    "            if self.np_random.random() < 0.3:\n",
    "                noise[2] = 0.0\n",
    "        goal += noise\n",
    "        return goal\n",
    "\n",
    "    def _sample_object(self) -> None:\n",
    "        object_position = np.array([0.0, 0.0, self.initial_object_height])\n",
    "        noise = self.np_random.uniform(self.obj_range_low, self.obj_range_high)\n",
    "        object_position += noise\n",
    "        object_xpos = np.concatenate([object_position, np.array([1, 0, 0, 0])])\n",
    "        self._utils.set_joint_qpos(self.model, self.data, \"obj_joint\", object_xpos)\n",
    "\n",
    "#############################  Setting  ###################################\n",
    "    def set_mocap_pose(self, position, orientation) -> None:\n",
    "        self._utils.set_mocap_pos(self.model, self.data, \"panda_mocap\", position)\n",
    "        self._utils.set_mocap_quat(self.model, self.data, \"panda_mocap\", orientation)\n",
    "\n",
    "    def set_joint_neutral(self) -> None:\n",
    "        # assign value to arm joints\n",
    "        for name, value in zip(self.arm_joint_names, self.neutral_joint_values[0:7]):\n",
    "            self._utils.set_joint_qpos(self.model, self.data, name, value)\n",
    "        # assign value to finger joints\n",
    "        for name, value in zip(self.gripper_joint_names, self.neutral_joint_values[7:9]):\n",
    "            self._utils.set_joint_qpos(self.model, self.data, name, value)\n",
    "\n",
    "#############################  Movement  ##################################\n",
    "    def _set_action(self, action) -> None:\n",
    "        action = action.copy()\n",
    "        if not self.block_gripper:\n",
    "            pos_ctrl, gripper_ctrl = action[:3], action[3]\n",
    "            fingers_ctrl = gripper_ctrl * 0.2\n",
    "            fingers_width = self.get_fingers_width().copy() + fingers_ctrl\n",
    "            fingers_half_width = np.clip(fingers_width / 2, self.ctrl_range[-1, 0], self.ctrl_range[-1, 1])\n",
    "        elif self.block_gripper:\n",
    "            pos_ctrl = action\n",
    "            fingers_half_width = 0\n",
    "\n",
    "        # control the gripper\n",
    "        self.data.ctrl[-2:] = fingers_half_width\n",
    "        # control the end-effector\n",
    "        pos_ctrl *= 0.05\n",
    "        pos_ctrl += self.get_ee_position().copy()\n",
    "        pos_ctrl[2] = np.max((0, pos_ctrl[2]))\n",
    "\n",
    "        self.set_mocap_pose(pos_ctrl, self.grasp_site_pose)\n",
    "\n",
    "    def _get_obs(self) -> dict:\n",
    "        ee_position = self._utils.get_site_xpos(self.model, self.data, \"ee_center_site\").copy()\n",
    "        ee_velocity = self._utils.get_site_xvelp(self.model, self.data, \"ee_center_site\").copy() * self.dt\n",
    "\n",
    "        if not self.block_gripper:\n",
    "            fingers_width = self.get_fingers_width().copy()\n",
    "\n",
    "        # object position\n",
    "        object_position = self._utils.get_site_xpos(self.model, self.data, \"obj_site\").copy()\n",
    "        # object rotations\n",
    "        object_rotation = rotations.mat2euler(self._utils.get_site_xmat(self.model, self.data, \"obj_site\")).copy()\n",
    "        # object linear velocities\n",
    "        object_velp = self._utils.get_site_xvelp(self.model, self.data, \"obj_site\").copy() * self.dt\n",
    "        # object angular velocities\n",
    "        object_velr = self._utils.get_site_xvelr(self.model, self.data, \"obj_site\").copy() * self.dt\n",
    "\n",
    "        if not self.block_gripper:\n",
    "            observation = {\n",
    "                \"observation\": np.concatenate(\n",
    "                    [\n",
    "                        ee_position,\n",
    "                        ee_velocity,\n",
    "                        fingers_width,\n",
    "                        object_position,\n",
    "                        object_rotation,\n",
    "                        object_velp,\n",
    "                        object_velr,\n",
    "                    ]\n",
    "                ).copy(),\n",
    "                \"achieved_goal\": object_position.copy(),\n",
    "                \"desired_goal\": self.goal.copy(),\n",
    "            }\n",
    "        else:\n",
    "            observation = {\n",
    "                \"observation\": np.concatenate(\n",
    "                    [\n",
    "                        ee_position,\n",
    "                        ee_velocity,\n",
    "                        object_position,\n",
    "                        object_rotation,\n",
    "                        object_velp,\n",
    "                        object_velr,\n",
    "                    ]\n",
    "                ).copy(),\n",
    "                \"achieved_goal\": object_position.copy(),\n",
    "                \"desired_goal\": self.goal.copy(),\n",
    "            }\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    def compute_reward(self, achieved_goal, desired_goal, info) -> SupportsFloat:\n",
    "        d = self.goal_distance(achieved_goal, desired_goal)\n",
    "        if self.reward_type == \"sparse\":\n",
    "            return -(d > self.distance_threshold).astype(np.float32)\n",
    "        else:\n",
    "            return -d\n",
    "    \n",
    "    def _is_success(self, achieved_goal, desired_goal) -> bool:\n",
    "        d = self.goal_distance(achieved_goal, desired_goal)\n",
    "        return bool(d < self.distance_threshold)\n",
    "\n",
    "    def step(self, action) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        if np.array(action).shape != self.action_space.shape:\n",
    "            raise ValueError(\"Action dimension mismatch\")\n",
    "\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        self._set_action(action)\n",
    "        self._mujoco_step(action)\n",
    "        self._step_callback()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self.render()\n",
    "\n",
    "        observation = self._get_obs().copy()\n",
    "        info = {\"is_success\": self._is_success(observation[\"achieved_goal\"], self.goal)}\n",
    "        terminated = bool(info[\"is_success\"])\n",
    "        truncated = self.compute_truncated(observation[\"achieved_goal\"], self.goal, info)\n",
    "        reward = self.unwrapped.compute_reward(observation[\"achieved_goal\"], self.goal, info)\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "############################  Simulation  #################################\n",
    "    def _render_callback(self) -> None:\n",
    "        # visualize goal site\n",
    "        sites_offset = (self.data.site_xpos - self.model.site_pos).copy()\n",
    "        site_id = self._model_names.site_name2id[\"target\"]\n",
    "        self.model.site_pos[site_id] = self.goal - sites_offset[site_id]\n",
    "        self._mujoco.mj_forward(self.model, self.data)\n",
    "\n",
    "    def _mujoco_step(self, action: Optional[np.ndarray] = None) -> None:\n",
    "        for _ in range(10):\n",
    "            self._mujoco.mj_step(self.model, self.data, nstep=self.n_substeps)\n",
    "\n",
    "##############################  Reset  ####################################\n",
    "    def _reset_sim(self) -> bool:\n",
    "        self.data.time = self.initial_time\n",
    "        self.data.qvel[:] = np.copy(self.initial_qvel)\n",
    "        if self.model.na != 0:\n",
    "            self.data.act[:] = None\n",
    "        self.set_joint_neutral()\n",
    "        self.set_mocap_pose(self.initial_mocap_position, self.grasp_site_pose)\n",
    "        self._sample_object()\n",
    "        self._mujoco.mj_forward(self.model, self.data)\n",
    "        return True\n",
    "\n",
    "    def reset_mocap_welds(self, model, data) -> None:\n",
    "        if model.nmocap > 0 and model.eq_data is not None:\n",
    "            for i in range(model.eq_data.shape[0]):\n",
    "                if model.eq_type[i] == mujoco.mjtEq.mjEQ_WELD:\n",
    "                    # relative pose\n",
    "                    model.eq_data[i, 3:10] = np.array([0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])\n",
    "        self._mujoco.mj_forward(model, data)\n",
    "\n",
    "##############################  Callback  ####################################\n",
    "\n",
    "class EpisodeInfoCallback(BaseCallback):\n",
    "    def __init__(self, env, verbose=0):\n",
    "        super(EpisodeInfoCallback, self).__init__(verbose)\n",
    "        self.env = env\n",
    "        self.print_interval = 200\n",
    "        self.last_print_step = 0\n",
    "        self.initial_object_position = None\n",
    "        self.goal_position = None\n",
    "\n",
    "    def _on_rollout_start(self):\n",
    "        obs = self.env.unwrapped._get_obs()\n",
    "        self.initial_object_position = obs[\"achieved_goal\"]\n",
    "        self.goal_position = obs[\"desired_goal\"]\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if (self.n_calls - self.last_print_step) >= self.print_interval:\n",
    "            self.last_print_step = self.n_calls\n",
    "            self.print_info()\n",
    "        return True\n",
    "\n",
    "    def print_info(self):\n",
    "        obs = self.env.unwrapped._get_obs()\n",
    "        achieved_goal = obs[\"achieved_goal\"]\n",
    "        distance = self.env.unwrapped.goal_distance(achieved_goal, self.goal_position)\n",
    "\n",
    "        print(\"Target Object Position:\", self.goal_position)\n",
    "        print(\"Initial Object Position:\", self.initial_object_position)\n",
    "        print(\"Final Object Position:\", achieved_goal)\n",
    "        print(\"Distance between Object and Target:%.5f\" % distance)\n",
    "\n",
    "######################  Moving Environment  ###############################\n",
    "class FrankaPushEnv(FrankaEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_type,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_path = os.path.join(os.getcwd(), \"assets\", \"push.xml\"),\n",
    "            n_substeps=25,\n",
    "            reward_type=reward_type,\n",
    "            block_gripper=True,\n",
    "            distance_threshold=0.05,\n",
    "            goal_xy_range=0.3,\n",
    "            obj_xy_range=0.3,\n",
    "            goal_x_offset=0.0,\n",
    "            goal_z_range=0.0,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "register(\n",
    "    id=\"FrankaPushSparse\",\n",
    "    entry_point=FrankaPushEnv,\n",
    "    kwargs={\"reward_type\": \"sparse\"},\n",
    "    max_episode_steps=50,\n",
    ")\n",
    "\n",
    "class FrankaPickEnv(FrankaEnv):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reward_type,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_path = os.path.join(os.getcwd(), \"assets\", \"pick.xml\"),\n",
    "            n_substeps=25,\n",
    "            reward_type=reward_type,\n",
    "            block_gripper=False,\n",
    "            distance_threshold=0.05,\n",
    "            goal_xy_range=0.3,\n",
    "            obj_xy_range=0.3,\n",
    "            goal_x_offset=0.0,\n",
    "            goal_z_range=0.2,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "register(\n",
    "    id=\"FrankaPickSparse\",\n",
    "    entry_point=FrankaPickEnv,\n",
    "    kwargs={\"reward_type\": \"sparse\"},\n",
    "    max_episode_steps=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Training(SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPushSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# SAC agent\n",
    "model = SAC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6),\n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/sac_push/\", \n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/SAC_push.zip')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Training(DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPushSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# DDPG agent\n",
    "model = DDPG(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6), \n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/ddpg_push/\",\n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/DDPG_push.zip')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Push Training(TQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPushSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    "    n_critics=2,\n",
    "    n_quantiles=25,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# TQC agent\n",
    "model = TQC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6),\n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/tqc_push/\",\n",
    "    \n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/TQC_push.zip')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Training(SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPickSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# SAC agent\n",
    "model = SAC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6),\n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/sac_pick/\",\n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/SAC_pick.zip')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Training(DDPG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPickSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# DDPG agent\n",
    "model = DDPG(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6), \n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/ddpg_pick/\",\n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/DDPG_pick.zip')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick Training(TQC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrankaPickSparse\", render_mode=\"human\")\n",
    "\n",
    "# Dimension of the action space\n",
    "n_actions = env.action_space.shape[-1]\n",
    "\n",
    "# Action noise\n",
    "action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "# Network architecture and optimizer parameters\n",
    "policy_kwargs = dict(\n",
    "    net_arch=[256, 256, 256],\n",
    "    optimizer_class=th.optim.Adam,\n",
    "    n_critics=2,\n",
    "    n_quantiles=25,\n",
    ")\n",
    "\n",
    "# HER strategy\n",
    "goal_selection_strategy = 'future'\n",
    "n_sampled_goal = 4\n",
    "\n",
    "# TQC agent\n",
    "model = TQC(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    action_noise=action_noise,\n",
    "    replay_buffer_class=HerReplayBuffer,\n",
    "    replay_buffer_kwargs=dict(\n",
    "        n_sampled_goal=n_sampled_goal,\n",
    "        goal_selection_strategy=goal_selection_strategy,\n",
    "    ),\n",
    "    learning_rate=0.001,\n",
    "    batch_size=512,\n",
    "    buffer_size=int(1e6),\n",
    "    tau=0.05,\n",
    "    gamma=0.95,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/tqc_pick/\",\n",
    ")\n",
    "\n",
    "# Visualize the network\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "model.policy.to(device)\n",
    "obs, _ = env.reset()\n",
    "sample_input = {key: th.tensor(value).float().unsqueeze(0).to(device) for key, value in obs.items()}\n",
    "output = model.policy(sample_input)\n",
    "dot = make_dot(output, params=dict(model.policy.named_parameters()))\n",
    "display(dot)\n",
    "\n",
    "# Start training\n",
    "callback = EpisodeInfoCallback(env)\n",
    "model.learn(total_timesteps=200, callback=callback)\n",
    "\n",
    "model.save('model/TQC_pick.zip')\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_franka_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
